\documentclass[12pt]{homework}
\newcommand{\transprob}{\P{s_i}{s_{i-1}}}
\newcommand{\obsprob}{\P{w}{s}}
\title{}
\author{}
\date{}
\begin{document}
\maketitle
\header{CS4248}{Assignment 2}{U096883L Shawn Tan}
\section{Introduction}

This assignment requires us to build a Part-of-Speech (POS) tagger, using 
training data from part of the Penn Treebank. The method used in our approach 
has to employ a Hidden Markov Model (HMM). This entails learning from the 
training data a set of parameters required for the HMM. Table \ref{parameters} 
shows the various sets of data we have to collect for this particular 
assignment.

In the following sections, we will explain in detail how individual aspects of 
the HMM was created, outlining some of the technical difficulties faced. We also  
experiment with two simple smoothing techniques, Laplace (add-one) and 
Witten-Bell smoothing. The two techniques will be evaluated according to their 
precision, recall and F1 measures.

\begin{table}
	\begin{center}
	\begin{tabular}{l l}
		\hline
		Name			&	Description\\
		\hline
		$V$				&	all unique words\\
		$S$				&	all unique POS tags\\
		$\transprob$	&	transition probability from one POS tag to another\\
		$\obsprob$		&	probability of seeing a word given a POS tag\\
		\hline
	\end{tabular}
	\end{center}
	\label{parameters}\caption{Parameters for a POS tagger HMM}
\end{table}
\section{Learning from \texttt{sents.train}}
The \texttt{sents.train} dataset contains 39,832 lines, each word annotated with 
POS tags.

In order to extract the relevant information we count.

In order to obtain $\transprob$, we need to count the different number of times 
one tag is followed by another. For each line, we extract the POS for each 
token, leaving a list of POS tags. We then prepend a `\^' at the beginning, and 
a `\$' character at the end. This ensures that the probabilities for a given POS 
starting a sentence are also taken into account
 
\end{document}
